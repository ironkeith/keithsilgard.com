---
layout: default
title: A Faster Website
description: My site felt a little slow, so I spent a few hours speeding it up.
category: articles
---
A long time ago, I read [High Performance Web Sites](http://shop.oreilly.com/product/9780596529307.do) by Steve Souders. In it, the author discusses the value of a fast website, and a bunch of really easy techniques to ensure your site is as fast as it can be. With my newfound knowledge, I promptly never did anything to ever try and speed up a website (with the exception of using non-blocking JS). 

When I relaunched this site a few weeks ago, I was happy with how simple it was, but it felt sluggish. It's all static files (generated by [Jekyll](https://github.com/mojombo/jekyll)), so it really should have been a lot snappier \[[1](#footnotes)\]. I decided to knuckle down and see what I could do to fix it.

My first step was to add all of my static images to Amazon S3. I thought this would be pretty straight forward, but it ended up being a bit of a pain. To start, I [signed up for an S3 account](http://aws.amazon.com/), and created a bucket with all of my images in it. Because I'm pedantic, I also created a subdomain so that I'd have prettier URLs. Surprisingly (to me), adding images to S3 didn't do anything to make the site faster. To speed it up, I had to sign up for [CloudFront](http://aws.amazon.com/cloudfront/) and add my bucket as a "download" distribution. That got images downloading noticeably faster.

Next, I set up far-future expires headers for all of my files. Getting this up and running on my server was easy enough. I just had to SSH in and enable mod_expires, and add a few declarations to my .htaccess file \[[2](#footnotes)\]. Getting them set up on S3 was a pain.

By default, S3 doesn't set expiry headers, and if you want to set them yourself, you need to do so on upload. I searched around for an easy way to do this, but the solution actually came courtesy of [Panic](http://www.panic.com). Their app [Transmit](http://www.panic.com/transmit/) can also handle S3 quite seamlessly, and you can set custom headers in the preferences. 

<figure>
	<img src="http://static.keithsilgard.com/images/articles/a-faster-site/transmit-prefs.png" alt="Setting default Expires headers for Amazon S3 in Panic's Transmit">
	<figcaption>Setting default Expires headers for Amazon S3 in Panic's Transmit</figcaption>
</figure>

The final step was setting up gzip compression, which was again, as easy as enabling mod_deflate, updating my .htaccess file \[[2](#footnotes)\], and reloading Apache. 

In all, I spent a few hours playing with this, and my site is noticeably faster. I now have an A \[[3](#footnotes)\] in [YSlow](http://developer.yahoo.com/yslow/), and 92/100 on [Google Page Speed](http://code.google.com/speed/page-speed/). If I would have known how easy making a site responsive is, I would have been doing it all along. It actually took me quite a bit longer to write this post than it did to dramatically improve my page response times.

<hr id="footnotes">

1.	Even though it's running off of a tiny little Linode VPS
2.	You can see my .htaccess file here: [https://github.com/ironkeith/keithsilgard.com/blob/master/.htaccess](https://github.com/ironkeith/keithsilgard.com/blob/master/.htaccess)	
3.	YSlow doesn't detect that static.keithsilgard.com is using a CDN, so you need to let it know to get that A